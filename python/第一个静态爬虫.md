# 第一个静态爬虫

## 爬取目标
此次爬取的目标是豆瓣读书页面的“最受关注图书榜”中的图书名称。url为"https://book.douban.com/"，要爬取的信息的截图如下：

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/douban2.PNG">
</div>

## 使用工具

- python 2.7
- beautifulsoup
- requests
- 谷歌浏览器

爬取工具可以选择使用urllib或者requests，它们最主要的区别是在爬取数据的时候连接方式的不同。urllb爬取完数据是直接断开连接的，而requests爬取数据之后可以继续复用socket，并没有断开连接。这里选择的是requets。beaurtifulsoup可以从HTML或XML文件中提取数据的Python库，简单来说，它能将HTML的标签文件解析成树形结构，然后方便地获取到指定标签的对应属性。

爬取的过程主要分为一下几部分：获得headers和cookies；使用requests爬取页面；使用beautifulsoup解析页面，得到数据。

## 获得headers和cookies
因为需要模拟http请求，所以需要时用headers和cookies。如何获得headers和cookies呢？首先使用浏览器转到豆瓣的登录页面https://accounts.douban.com/passport/login，之后按F12打开谷歌浏览器的开发者界面，选择network，xhr，如下图所示。然后输入自己的用户名和密码，点击登录，就会看到生成了一系列的报文。点击报文，寻找headers和cookies。这里在review_recommend?user_id=195568288中找到了。headers和cookies可能不止存在一处，比如这里的ebooks?user_id=195568288中也存在，如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/douban1.PNG">
</div>

然后，将headers和cookies复制出来，格式化成两个dict(可以使用sublime的格式化功能)，如下所示：

```python
cookie = { 
    'll':'108288',
    'bid':'a-gD3wCQcAQ',
    '_vwo_uuid_v2':'D5B091F82965EE6FFD2537B7CB67F0E84|626ebc56cf82e7c7c7687f0c1e2f3f43',
    'douban-fav-remind':'1',
    'gr_user_id':'537f64ef-083c-4d59-9b10-07a87e69e043',
    'gr_cs1_4934a955-25db-4912-8020-fd66f755cc20':'user_id%3A0',
    '_pk_ref.100001.3ac3':'%5B%22%22%2C%22%22%2C1556274990%2C%22https%3A%2F%2Fwww.google.com.hk%2F%22%5D',
    '_pk_ses.100001.3ac3':'*',
    'ap_v':'0,6.0',
    '__yadk_uid':'iIm0UPM11HIatvnmyKOuUVGXFF5Ggsfw',
    '__utma':'30149280.493188498.1554890669.1555656459.1556274991.4',
    '__utmc':'30149280',
    '__utmz':'30149280.1556274991.4.4.utmcsr: google|utmccn: (organic)|utmcmd: organic|utmctr: (not%20provided)',
    '__utma':'81379588.1914928062.1556274991.1556274991.1556274991.1',
    '__utmc':'81379588',
    '__utmz':'81379588.1556274991.1.1.utmcsr: google|utmccn: (organic)|utmcmd: organic|utmctr: (not%20provided)',
    '__utmb':'81379588.1.10.1556274991',
    '__utmt':'1',
    'dbcl2':'195568288:z2TVs+Biu8c',
    'ck':'xiFq',
    'push_noty_num':'0',
    'push_doumail_num':'0',
    '__utmv':'30149280.19556',
    '__utmb':'30149280.4.10.1556274991',
    'gr_session_id_22c937bbd8ebd703f2d8e9445f7dfd03':'d4bcf8e5-ab26-43ab-8253-9c9d69a4d0a6',
    'gr_cs1_d4bcf8e5-ab26-43ab-8253-9c9d69a4d0a6':'user_id%3A1',
    '_pk_id.100001.3ac3':'11062d4fd097d946.1556274990.1.1556275814.1556274990.',
    'gr_session_id_22c937bbd8ebd703f2d8e9445f7dfd03_d4bcf8e5-ab26-43ab-8253-9c9d69a4d0a6':'true'
}

header = { 
    'Accept':'*/*',
    'Accept-Encoding':'gzip, deflate, br',
    'Accept-Language':'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control':'no-cache',
    'Connection':'keep-alive',
    'Host':'book.douban.com',
    'Pragma':'no-cache',
    'Referer':'https://book.douban.com/',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36',
    'X-Requested-With':'XMLHttpRequest',
}
```

## 爬取和转换
首先是爬取，之后将爬取的网页代码写入文件中。代码如下。
```python
    url = "https://book.douban.com/"
    response = requests.get(url, headers=header, cookies=cookie)
    soup = BeautifulSoup(response.content, "html.parser")

    with open("/home/my-scripts/douban_spider.html", "w+") as out:
        out.write(bytes(soup))
```
这里将爬取的网页代码写入douban_spider.html文件中。一般来说，爬取的文网页格式会比较混乱，可以通过在线html转换工具进行格式化， 地址为[在线html格式化](https://www.freeformatter.com/html-formatter.html#ad-output)。最后生成的html文件在[这里](https://github.com/adamhand/LeetCode-images/blob/master/douban_spider.html)。

之后就是分析代码并提取自己需要的信息。打开下载的html文件，，可以看到它是由一个一个的标签组成的，标签就是形如`<li>`、`<div>`的标志，标签之间往往还有多层嵌套。提取关键字的一般原则是从里往外，即如果最里面的标签能够定位到关键字，就直接取里面的标签，之后再从标签里提取关键字。否则就提取外层标签，即父标签，知道能够比较准确地定位到关键字位置。

经过分析，要提取的关键字是由一个个`<li>`标签包裹的，如下图所示：

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/douban3.PNG">
</div>

因为只需要提取书的名字，所以定位标签选择的是`<h4>`。代码如下：

```python
    soup_h4 = soup.find_all('h4', class_='title')
    
    with open("/home/my-scripts/douban_h4.html", "w+") as out:
        for _ in soup_h4:
            out.write(_.encode('utf-8'))
```
将提取结果写入douban_h4.html文件中，格式化后的文件在[这里](https://github.com/adamhand/LeetCode-images/blob/master/douban_h4.html)。

接着就是提取关键字，这里用了一个比较笨的方法，使用str的split方法，将提取的结果写入h4_1.txt中。代码如下：

```python
    for h4 in soup_h4:
        h4_1 = h4.find('a', class_='')
        if h4_1 is None:
            continue
        else:
            h4_1 = h4_1.encode('utf-8')
            h4_1 = h4_1.split('\'})\">')[1]
            h4_1 = h4_1.split('</')[0]
            print h4_1

            with open("/home/my-scripts/h4_1.txt", "a+") as f:
                f.write(h4_1+"\n")
```

总的代码如下所示。

```python
def douban_spider():
    url = "https://book.douban.com/"
    response = requests.get(url, headers=header, cookies=cookie)
    soup = BeautifulSoup(response.content, "html.parser")

    with open("/home/my-scripts/douban_spider.html", "w+") as out:
        out.write(bytes(soup))
    
    soup_h4 = soup.find_all('h4', class_='title')
    
    with open("/home/my-scripts/douban_h4.html", "w+") as out:
        for _ in soup_h4:
            out.write(_.encode('utf-8'))

    for h4 in soup_h4:
        h4_1 = h4.find('a', class_='')
        if h4_1 is None:
            continue
        else:
            h4_1 = h4_1.encode('utf-8')
            h4_1 = h4_1.split('\'})\">')[1]
            h4_1 = h4_1.split('</')[0]
            print h4_1

            with open("/home/my-scripts/h4_1.txt", "a+") as f:
                f.write(h4_1+"\n")



def main():
    douban_spider()


if __name__ == "__main__":
    main()
```

爬取的结果存在了h4_1.txt里，内容为：
```
世间万物
一个人生活
六里庄遗事
皮肤的秘密
守夜
观看王维的十九种方式
奥丽芙·基特里奇
写作的禅机
推理竞技场
这世界如露水般短暂
```

## 遇到的问题
一开始存储soup_h4的时候遇到了中文编码的问题，但是soup中的中文却正常显示。也就是说，使用BeautifulSoup()方法之后，显示没问题，使用find_all()方法之后，中文显示的是unicode码，而不是汉字。一开始存储soup_h4的代码如下所示，存储的文件在[这里](https://github.com/adamhand/LeetCode-images/blob/master/douban_h4_fault.html)：

```python
   with open("/home/my-scripts/douban_h4.html", "w+") as out:
        out.write(bytes(soup_h4))
```
找了很多资料，大部分人遇到的是中文乱码的问题，但是这里不是乱码，编码是正确的。最后找到的原因是，find_all()函数返回的是一个list，而python列表中不能输出汉字，只能单独输出。所以，改为如下的代码，就可以了。

```python
    with open("/home/my-scripts/douban_h4.html", "w+") as out:
        for _ in soup_h4:
            out.write(_.encode('utf-8'))
```

另外，在取soup_h4中的内容，即最终想要的书名的时候，也遇到了问题，一开始的写法如下：

```python
   for h4 in soup_h4:
        h4_1 = h4.find('a', class_='')
        if h4_1 is None:
            continue
        else:
            print h4_1.contents
```
这种写法也是输出一堆unicode码，后来试着改为`str(h4_1.contents).encode('utf-8')`，想对contents进行编码，输出的还是unicode码。最后使用了一个笨方法，使用字符串的split()方法对关键字进行提取。

另外，关于BeautifulSoup的编码，这里小记一下。requests和beautifulsoup都会自行猜测原文的编码方式，然后用猜测出来的编码方式进行解码转换成unicode。大多数时候猜测出来的编码都是正确的，但也有猜错的情况，如果猜错了可以指定原文的编码。查看requests和BeautifulSoup使用的编码方法为：

```python
response = requests.get(url, headers=header, cookies=cookie)
print response.apparent_encoding  # 查看requests使用的编码格式，两种方法都可以
print response.encoding

soup = BeautifulSoup(response.content, "html.parser")
print soup.original_encoding   # 查看BeautifulSoup使用的编码格式
```
得到requests和BeautifulSoup的编码方式后，查看要爬取的原网站(这里是豆瓣)，查看它的`charset`属性，如果和requests和BeautifulSoup的编码方式相同，说明是正确的，否则就要指定requests和BeautifulSoup的编码方式。指定方法如下：

```python
response = requests.get(url, headers=header, cookies=cookie)
response.encoding='gb18030'  # requests的结果使用gb18030编码，和原网站的编码相同
soup = BeautifulSoup(response.content, "lxml", from_encoding='gb18030')  # BeautifulSoup指定使用gb18030编码
```

另外，BeautifulSoup输出的时候会默认使用`utf-8`对`unicode`进行编码，如果原文不是以`utf-8`进行编码的话，输出还是会乱码。这时候，可以用prettify（）或则encode（）方式来指定编码。

```
soup = BeautifulSoup(response.content, "html.parser")
print soup.title.prettify('gb18030')
print soup.title.encode('gb18030')
```

## 注意
另外需要注意一点，爬取的时候浏览器这边要保持登录，否则可能会出现"requests.exceptions.TooManyRedirects: Exceeded 30 redirects"，网页重定向的问题。

## 参考
[Python 使用 Beautiful Soup 抓取與解析網頁資料，開發網路爬蟲教學](https://blog.gtwang.org/programming/python-beautiful-soup-module-scrape-web-pages-tutorial/)

[BeautifulSoup全面总结](https://zhuanlan.zhihu.com/p/35354532)

[python使用beautifulsoup乱码问题](https://segmentfault.com/q/1010000004998311/a-1020000004998595)

[爬虫笔记（三）：BeautifulSoup基础](http://zangbo.me/2017/06/15/Crawler_3/)

[requests和BeautifulSoup中文编码转换心得](https://www.jianshu.com/p/69401b84419e)

[Python爬虫之爬取静态网页](https://www.jianshu.com/p/f8516eb9913f)
