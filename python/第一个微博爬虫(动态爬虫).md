# 第一个微博爬虫（动态网页爬虫）

## 爬取目标
微博游戏品类账号和ID爬取。地址为https://d.weibo.com/1087030002_2975_2019_0#。如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo1(1).png">
</div>

## 工具
- scrapy
- python 2.7
- 谷歌浏览器

因为这次是要抓取多页信息，使用scrapy这个爬虫框架比较方便，它的start_requests()方法可以很容易实现循环抓取的功能。

## 分析
一开始按照静态爬虫的那一套流程进行爬取，url为https://d.weibo.com/1087030002_2975_2019_0#，发现爬取的内容中有很多没有用的信息，网页中其他部分的内容也被爬了下来，分析起来很困难。所以就要进一步缩小爬取的范围，这也是动态爬取和静态爬取的一个不同点，可能还要再原始要爬取的url上再分析，缩小范围。

## 分析url
还是来到https://d.weibo.com/1087030002_2975_2019_0#页面，F12打开开发者模式，点击clear按钮将现有的response清除掉，避免干扰分析，并选择`doc`如下图所所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo2.PNG">
</div>

之后点击下一页，捕获新的response。如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo3.PNG">
</div>

这时，新的response就被捕获了，点击它在右侧查看它的内容，会发现需要的信息全部包含在里面。而且这时一个被script包裹的json字符串。如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo4(1).PNG">
</div>

之后再response报文上右键->copy->copy as cURL(bash)，如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo5.png">
</div>

这时，打开linux系统的一个终端，将复制的内容粘贴进去，回车之后就会发现，返回的正是右侧被script包裹着的json字符串。这样就对了，需要爬取的真实url就找到了。将复制的curl粘贴到sublime中，内容如下：

```python
curl 'https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=1&ajaxpagelet=1&__ref=/1087030002_2975_2019_0&_t=FM_155617905181422' -H 'Connection: keep-alive' -H 'Pragma: no-cache' -H 'Cache-Control: no-cache' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3' -H 'Referer: https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=1' -H 'Accept-Encoding: gzip, deflate, br' -H 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8' -H 'Cookie: SINAGLOBAL=4745954880587.613.1543981262479; SCF=AkScxNIp5d5cbe_NvU4kXNUNSJfT4IxwFSxAWd_VpRrVTOR1oQitoFw2FuNtLDcwPXfRhqsVsQ8XiJotaQ2tLis.; SUHB=0KAg3TKv1WXIM9; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9WWzl3Mhb3RL9-O5VqxmIJkB; UOR=,,running.webdev.com; SUB=_2AkMr6fj8f8NxqwJRmP0Wz2zgZYh0zQHEieKdtQknJRMxHRl-yj9jqlIbtRB6AGnWE6WfL431TSvWRj27ovYWvmk9Y4wN; login_sid_t=2b493496a69c5afa3ab081acc7531a46; cross_origin_proto=SSL; _s_tentry=-; Apache=7440462133996.797.1555921968660; ULV=1555921968677:10:3:1:7440462133996.797.1555921968660:1555297356534; _T_WM=a8151c9f5d01090ddea87e5b8164331e; YF-Page-G0=237c624133c0bee3e8a0a5d9466b74eb|1556178529|1556178309; TC-Page-G0=589da022062e21d675f389ce54f2eae7|1556179137|1556178944' --compressed
```
整个curl地址可以被分为三部分，第一部分是需要爬取的真实url，即`'https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=1&ajaxpagelet=1&__ref=/1087030002_2975_2019_0&_t=FM_155617905181422'`；第二部分是headers，即`-H`后面的那一坨，如下。

```python
-H 'Connection: keep-alive' -H 'Pragma: no-cache' -H 'Cache-Control: no-cache' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3' -H 'Referer: https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=1' -H 'Accept-Encoding: gzip, deflate, br' -H 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8'
```

第三部分是cookies，在随后一个`-H`后面，即：
```python
'Cookie: SINAGLOBAL=4745954880587.613.1543981262479; SCF=AkScxNIp5d5cbe_NvU4kXNUNSJfT4IxwFSxAWd_VpRrVTOR1oQitoFw2FuNtLDcwPXfRhqsVsQ8XiJotaQ2tLis.; SUHB=0KAg3TKv1WXIM9; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9WWzl3Mhb3RL9-O5VqxmIJkB; UOR=,,running.webdev.com; SUB=_2AkMr6fj8f8NxqwJRmP0Wz2zgZYh0zQHEieKdtQknJRMxHRl-yj9jqlIbtRB6AGnWE6WfL431TSvWRj27ovYWvmk9Y4wN; login_sid_t=2b493496a69c5afa3ab081acc7531a46; cross_origin_proto=SSL; _s_tentry=-; Apache=7440462133996.797.1555921968660; ULV=1555921968677:10:3:1:7440462133996.797.1555921968660:1555297356534; _T_WM=a8151c9f5d01090ddea87e5b8164331e; YF-Page-G0=237c624133c0bee3e8a0a5d9466b74eb|1556178529|1556178309; TC-Page-G0=589da022062e21d675f389ce54f2eae7|1556179137|1556178944'
```

至此为止，请求的url，headers和cookies都找到了，使用sublime的正则功能对headers和cookies进行格式化，如下：

```python
header = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "DNT": "1",
    "Pragma": "no-cache",
    "Referer": "https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=3",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36"
}

cookie = {
    "Apache": "5680501708554.002.1556247904067",
    "SINAGLOBAL": "5680501708554.002.1556247904067",
    "SUB": "_2AkMrnvppf8NxqwJRmfgWxGnibIpxyw3EieKdwguyJRMxHRl-yT9jqlEZtRB6AB7UhgCDNyShwz9sawTBDbVp-d1nOZJE",
    "SUBP": "0033WrSXqPxfM72-Ws9jqgMF55529P9D9WhKGgTlADqOhuVB.nsXaQn3",
    "TC-Page-G0": "c427b4f7dad4c026ba2b0431d93d839e|1556247906|1556247898",
    "ULV": "1556247904091:1:1:1:5680501708554.002.1556247904067:",
    "_s_tentry": "passport.weibo.com"
}
```

但是还有一个需求，是爬取账号所属的子分类，所以还需要分析一下子分类对爬取url的影响。刚才的所有分析都是在分类为“全部的情况下做的，如下图所示。

<div align="center">
<img src="https://raw.githubusercontent.com/adamhand/LeetCode-images/master/weibo6.PNG">
</div>

现在只需要选择其他分类，比如“游戏选手”，再次按照上面的步骤分析一下url即可，最后分析发现“游戏选手”下的url和“全部”下的url分别为：

```python
'https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=1&ajaxpagelet=1&__ref=/1087030002_2975_2019_0&_t=FM_155617905181422'

'https://d.weibo.com/1087030002_2975_2019_9?pids=Pl_Core_F4RightUserList__4&page=1&ajaxpagelet=1&__ref=/1087030002_2975_2019_0&_t=FM_155617905181422'
```
可以发现，只有`//d.weibo.com/1087030002_2975_2019_9`中最有一个数字是不一样的，所以，有理由相信，每个分类的这个位置的数字都不一样，这个数字就是分类的标志。将所有的分类的该部分和对应的分类名组成一个dict，在请求的时候将该部分插入url的相应位置即可。如下所示：

```python
catalog_map = {
    '//d.weibo.com/1087030002_2976_2019_9': '游戏选手',
    '//d.weibo.com/1087030002_2976_2019_17': '游戏战队',
    '//d.weibo.com/1087030002_2976_2019_8': '游戏解说',
    '//d.weibo.com/1087030002_2976_2019_11': '游戏攻略',
    '//d.weibo.com/1087030002_2976_2019_1': '手机游戏',
    '//d.weibo.com/1087030002_2976_2019_6': '单机游戏',
    '//d.weibo.com/1087030002_2976_2019_7': '网络游戏',
    '//d.weibo.com/1087030002_2976_2019_13': '电子竞技',
    '//d.weibo.com/1087030002_2976_2019_16': 'Dota',
    '//d.weibo.com/1087030002_2976_2019_14': 'LOL',
    '//d.weibo.com/1087030002_2976_2019_19': '魔兽',
    '//d.weibo.com/1087030002_2976_2019_15': '炉石传说',
    '//d.weibo.com/1087030002_2976_2019_10': '游戏公司',
    '//d.weibo.com/1087030002_2976_2019_21': '主机游戏',
    '//d.weibo.com/1087030002_2976_2019_23': '游戏从业者'
}
```
下面就开始撸代码了，直接给出所有的代码。依然是爬取之后进行分析。因为爬取的是一个script包裹的json，所以处理的流程是首先通过replace函数将script进行替换，得到里面的json，之后提取json里面的html，最后就可以对html进行分析了。

最终得到的html代码在[这里](https://github.com/adamhand/LeetCode-images/blob/master/weibo.html)。

## 代码
```python
# _*_ coding=utf-8 _*_
import scrapy
import time
from scrapy.selector import Selector
import json
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

header = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "DNT": "1",
    "Pragma": "no-cache",
    "Referer": "https://d.weibo.com/1087030002_2975_2019_0?pids=Pl_Core_F4RightUserList__4&page=3",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36"
}

cookie = {
    "Apache": "5680501708554.002.1556247904067",
    "SINAGLOBAL": "5680501708554.002.1556247904067",
    "SUB": "_2AkMrnvppf8NxqwJRmfgWxGnibIpxyw3EieKdwguyJRMxHRl-yT9jqlEZtRB6AB7UhgCDNyShwz9sawTBDbVp-d1nOZJE",
    "SUBP": "0033WrSXqPxfM72-Ws9jqgMF55529P9D9WhKGgTlADqOhuVB.nsXaQn3",
    "TC-Page-G0": "c427b4f7dad4c026ba2b0431d93d839e|1556247906|1556247898",
    "ULV": "1556247904091:1:1:1:5680501708554.002.1556247904067:",
    "_s_tentry": "passport.weibo.com"
}
catalog_map = {
    '//d.weibo.com/1087030002_2976_2019_9': '游戏选手',
    '//d.weibo.com/1087030002_2976_2019_17': '游戏战队',
    '//d.weibo.com/1087030002_2976_2019_8': '游戏解说',
    '//d.weibo.com/1087030002_2976_2019_11': '游戏攻略',
    '//d.weibo.com/1087030002_2976_2019_1': '手机游戏',
    '//d.weibo.com/1087030002_2976_2019_6': '单机游戏',
    '//d.weibo.com/1087030002_2976_2019_7': '网络游戏',
    '//d.weibo.com/1087030002_2976_2019_13': '电子竞技',
    '//d.weibo.com/1087030002_2976_2019_16': 'Dota',
    '//d.weibo.com/1087030002_2976_2019_14': 'LOL',
    '//d.weibo.com/1087030002_2976_2019_19': '魔兽',
    '//d.weibo.com/1087030002_2976_2019_15': '炉石传说',
    '//d.weibo.com/1087030002_2976_2019_10': '游戏公司',
    '//d.weibo.com/1087030002_2976_2019_21': '主机游戏',
    '//d.weibo.com/1087030002_2976_2019_23': '游戏从业者'
}

class demoSpider(scrapy.Spider):
    name = "demospider"

    def start_requests(self):
        for catalog_url, catalog_name in catalog_map.items():
            for i in range(1, 20):
                url = "https:%s?pids=Pl_Core_F4RightUserList__4&" \
                      "page=%d" \
                      "&ajaxpagelet=1&__ref=/1087030002_2975_2019_0&_t=FM_155624790417118" % (catalog_url, i)
                yield scrapy.Request(url=url, headers=header, cookies=cookie, callback=self.parse,
                                     meta={'page_num': i, 'catalog_name': catalog_name})
                time.sleep(1)

    def parse(self, response):
        page = response.meta['page_num']
        catalog_name = response.meta['catalog_name']

        script = response.body
        json_str = script.replace('<script>parent.FM.view(', '').replace(')</script>', '').strip()
        data = json.loads(json_str)
        html = data['html']
        #print html
        doc = Selector(text=html)
        lis = doc.css('li.follow_item')
        if not lis:
            return
        for li in lis[1:]:
            usercard = li.css('a.S_txt1::attr(usercard)')[0].extract()
            usercard = str(usercard).split("&")[0]
            usercard = usercard.split("=")[1]
            name2 = li.css('strong::text')[0].extract()
            em_counts = li.css('span.conn_type>em.count::text')
            #print name2, page, catalog_name, [em.extract() for em in em_counts], usercard
            with open ("/home/my-scripts/demoweibo.txt", "a+") as out:
                #out.write(name2+" "+bytes(page)+" "+ catalog_name+" "+usercard+"\n")
                out.write(name2+" "+ catalog_name+" "+usercard+"\n")
```

## 补充
谷歌浏览器开发者模式的各种功能简单小结。

开发者工具包括 Elements 、 Console 、 Sources 、 Network 等多个标签页，分别提供了以下功能：

- Elements：显示网页经过渲染之后的结构，可以任意调整和修改网页元素，并即时显示修改结果；
- Console：打印变量信息，用于代码调试，网页运行过程中产生的警告和报错也会出现在这里；
- Sources：查看网页所使用到的全部资源文件；
- Network：查看网页所请求的各类资源文件及其对应的请求时间。

Network 标签页会记录网页在渲染过程中所请求的各类资源文件及其对应的请求时间。大多数网页只在一开始加载的时候请求各类资源文件，加载完毕后不再请求；也有一些网页在加载完毕后仍定时请求一些资源，用于动态更新页面上的内容。所访问的网页使用了哪些资源，用户浏览的过程中网页做了哪些事情，这些都可以在 Network 标签页中找到答案。

Network 标签页中的资源文件主要分为以下几大类：

- All：不加筛选条件，所请求的全部资源文件；
- XHR：异步请求的数据；
- JS：js代码文件；
- CSS：css样式文件；
- Img：jpg、png等图片文件；
- Media：媒体资源文件；
- Font：字体文件；
- Doc：静态html文档。

## 参考
[如何爬动态加载的页面？ajax爬虫你有必要掌握](https://mjzj.com/article/37403)

[全栈 - 7 爬虫 Http请求和Chrome](https://zhuanlan.zhihu.com/p/25017443)

